# 229PokerProphet
Poker solvers analyze table situations and give analyses of effectiveness on different actions per game theory optimal (GTO)â€“but are unable to make definitive decisions. As a result, bots turn to behavior cloning founded in supervised learning algorithms. However, we propose that reinforcement learning algorithms can see even greater success on exploitative playing. Initial results show that self-exploration models see consistent exploitative wins over traditional supervised learning models through polling multiple simultaneously self-optimized models.

Implementing and comparing supervised (logistic regression; k-nearest neighbors) and reinforcement learning (self play; deep-Q -> ensemble sampling) strategies for playing poker exploitatively.
